---
---

@string{aps = {American Physical Society,}}

@misc{moenneloccoz20243dgaussianraytracing,
    title={3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes}, 
    author={Nicolas Moenne-Loccoz and Ashkan Mirzaei and Or Perel and Riccardo de Lutio and Janick Martinez Esturo and Gavriel State and Sanja Fidler and Nicholas Sharp and Zan Gojcic},
    journal={SIGGRAPH Asia},
    year={2024},
    eprint={2407.07090},
    archivePrefix={arXiv},
    primaryClass={cs.GR},
    url={https://arxiv.org/abs/2407.07090}, 
    pdf={https://arxiv.org/abs/2407.07090.pdf},
    abstract={Particle-based representations of radiance fields such as 3D Gaussian Splatting have found great success for reconstructing and re-rendering of complex scenes. Most existing methods render particles via rasterization, projecting them to screen space tiles for processing in a sorted order. This work instead considers ray tracing the particles, building a bounding volume hierarchy and casting a ray for each pixel using high-performance GPU ray tracing hardware. To efficiently handle large numbers of semi-transparent particles, we describe a specialized rendering algorithm which encapsulates particles with bounding meshes to leverage fast ray-triangle intersections, and shades batches of intersections in depth-order. The benefits of ray tracing are well-known in computer graphics: processing incoherent rays for secondary lighting effects such as shadows and reflections, rendering from highly-distorted cameras common in robotics, stochastically sampling rays, and more. With our renderer, this flexibility comes at little cost compared to rasterization. Experiments demonstrate the speed and accuracy of our approach, as well as several applications in computer graphics and vision. We further propose related improvements to the basic Gaussian representation, including a simple use of generalized kernel functions which significantly reduces particle hit counts.},
    img={3dgrt.png},
    website={https://gaussiantracer.github.io/},
    selected={true}
}

@article{modi2024Simplicits,
    title={Simplicits: Mesh-Free, Geometry-Agnostic, Elastic Simulation}, 
    author={Vismay Modi, Nicholar Sharp, Or Perel, David I. W. Levin, Shinjiro Sueda},
    journal={SIGGRAPH},
    year={2024},
    url={https://arxiv.org/abs/2407.09497},
    pdf={https://arxiv.org/abs/2407.09497.pdf},
    abstract={The proliferation of 3D representations, from explicit meshes to implicit neural fields and more, motivates the need for simulators agnostic to representation. We present a data-, mesh-, and grid-free solution for elastic simulation for any object in any geometric representation undergoing large, nonlinear deformations. We note that every standard geometric representation can be reduced to an occupancy function queried at any point in space, and we define a simulator atop this common interface. For each object, we fit a small implicit neural network encoding spatially varying weights that act as a reduced deformation basis. These weights are trained to learn physically significant motions in the object via random perturbations. Our loss ensures we find a weight-space basis that best minimizes deformation energy by stochastically evaluating elastic energies through Monte Carlo sampling of the deformation volume. At runtime, we simulate in the reduced basis and sample the deformations back to the original domain. Our experiments demonstrate the versatility, accuracy, and speed of this approach on data including signed distance functions, point clouds, neural primitives, tomography scans, radiance fields, Gaussian splats, surface meshes, and volume meshes, as well as showing a variety of material energies, contact models, and time integration schemes.},
    img={simplicits.gif},
    website={https://research.nvidia.com/labs/toronto-ai/simplicits/},
    selected={true}
}

@article{mikaeili2023sked,
      title={SKED: Sketch-guided Text-based 3D Editing}, 
      author={Aryan Mikaeili and Or Perel and Mehdi Safaee and Daniel Cohen-Or and Ali Mahdavi-Amiri},
      journal={ICCV},       
      year={2023},
      eprint={2303.10735},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.10735},
      pdf={https://arxiv.org/abs/2303.10735.pdf},
      abstract={Text-to-image diffusion models are gradually introduced into computer graphics, recently enabling the development of Text-to-3D pipelines in an open domain. However, for interactive editing purposes, local manipulations of content through a simplistic textual interface can be arduous. Incorporating user guided sketches with Text-to-image pipelines offers users more intuitive control. Still, as state-of-the-art Text-to-3D pipelines rely on optimizing Neural Radiance Fields (NeRF) through gradients from arbitrary rendering views, conditioning on sketches is not straightforward. In this paper, we present SKED, a technique for editing 3D shapes represented by NeRFs. Our technique utilizes as few as two guiding sketches from different views to alter an existing neural field. The edited region respects the prompt semantics through a pre-trained diffusion model. To ensure the generated output adheres to the provided sketches, we propose novel loss functions to generate the desired edits while preserving the density and radiance of the base instance. We demonstrate the effectiveness of our proposed method through several qualitative and quantitative experiments.},
      img={sked.png},
      website={https://sked-paper.github.io/},
      selected={true}
}

@article{hertz2022spaghetti, 
         title={SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation}, 
         author={Hertz, Amir and Perel, Or and Giryes, Raja and Sorkine-Hornung, Olga and Cohen-Or, Daniel}, 
         journal={SIGGRAPH}, 
         year={2022},
         archivePrefix={arXiv},
         url={https://arxiv.org/abs/2201.13168},
         pdf={https://arxiv.org/abs/2201.13168.pdf},
         abstract={Neural implicit fields are quickly emerging as an attractive representation for learning based techniques. However, adopting them for 3D shape modeling and editing is challenging. We introduce a method for Editing Implicit Shapes Through Part Aware GeneraTion, permuted in short as SPAGHETTI. Our architecture allows for manipulation of implicit shapes by means of transforming, interpolating and combining shape segments together, without requiring explicit part supervision. SPAGHETTI disentangles shape part representation into extrinsic and intrinsic geometric information. This characteristic enables a generative framework with part-level control. The modeling capabilities of SPAGHETTI are demonstrated using an interactive graphical interface, where users can directly edit neural implicit shapes.},
         img={spaghetti.png},
         code={https://github.com/amirhertz/spaghetti},
         website={https://amirhertz.github.io/spaghetti/},
         selected={true}
}

@article{hertz2021meshdrape,
  author = {Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
  title = {Mesh Draping: Parametrization-Free Neural Mesh Transfer},
  journal = {Computer Graphics Forum},
  volume = {42},
  number = {1},
  pages = {72-85},
  keywords = {shape synthesis, shape modelling, neural networks},
  doi = {https://doi.org/10.1111/cgf.14721},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14721},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14721},
  pdf={https://arxiv.org/abs/2110.05433.pdf},
  abstract = {Despite recent advances in geometric modelling, 3D mesh modelling still involves a considerable amount of manual labour by experts. In this paper, we introduce Mesh Draping: a neural method for transferring existing mesh structure from one shape to another. The method drapes the source mesh over the target geometry and at the same time seeks to preserve the carefully designed characteristics of the source mesh. At its core, our method deforms the source mesh using progressive positional encoding (PE). We show that by leveraging gradually increasing frequencies to guide the neural optimization, we are able to achieve stable and high-quality mesh transfer. Our approach is simple and requires little user guidance, compared to contemporary surface mapping techniques which rely on parametrization or careful manual tuning. Most importantly, Mesh Draping is a parameterization-free method, and thus applicable to a variety of target shape representations, including point clouds, polygon soups and non-manifold meshes. We demonstrate that the transferred meshing remains faithful to the source mesh design characteristics, and at the same time fits the target geometry well.},
  year = {2023},
  img={meshdrape.jpg},
  selected={true}
}

@article{hertz2021meshdrape,
      title={Mesh Draping: Parametrization-Free Neural Mesh Transfer}, 
      author={Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
      journal={arxiv},
      year={2021},
      eprint={2110.05433},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2110.05433},
      pdf={https://arxiv.org/abs/2110.05433.pdf},
      abstract={Despite recent advances in geometric modeling, 3D mesh modeling still involves a considerable amount of manual labor by experts. In this paper, we introduce Mesh Draping: a neural method for transferring existing mesh structure from one shape to another. The method drapes the source mesh over the target geometry and at the same time seeks to preserve the carefully designed characteristics of the source mesh. At its core, our method deforms the source mesh using progressive positional encoding. We show that by leveraging gradually increasing frequencies to guide the neural optimization, we are able to achieve stable and high quality mesh transfer. Our approach is simple and requires little user guidance, compared to contemporary surface mapping techniques which rely on parametrization or careful manual tuning. Most importantly, Mesh Draping is a parameterization-free method, and thus applicable to a variety of target shape representations, including point clouds, polygon soups, and non-manifold meshes. We demonstrate that the transferred meshing remains faithful to the source mesh design characteristics, and at the same time fits the target geometry well.},
      img={meshdrape.jpg},
      selected={false}
}

@article{hertz2021sape,
      title={SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization}, 
      author={Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
      journal={NeurIPS},
      year={2021},
      eprint={2104.09125},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2104.09125},
      pdf={https://arxiv.org/pdf/2104.09125.pdf},
      abstract={Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.},
      img={sape.jpg},
      code={https://github.com/amirhertz/SAPE},
      website={https://amirhertz.github.io/sape/},
      selected={true}
}

@article{DBLP:journals/corr/abs-2103-10139,
  author    = {Or Perel and
               Oron Anschel and
               Omri Ben{-}Eliezer and
               Shai Mazor and
               Hadar Averbuch{-}Elor},
  title     = {Learning Multimodal Affinities for Textual Editing in Images},
  journal   = {Transaction on Graphics, Presented at SIGGRAPH},
  volume    = {abs/2103.10139},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.10139},
  pdf       = {https://arxiv.org/pdf/2103.10139.pdf},
  archivePrefix = {arXiv},
  eprint    = {2103.10139},
  timestamp = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-10139.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract  = {Nowadays, as cameras are rapidly adopted in our daily routine, images of documents are becoming both abundant and prevalent. Unlike natural images that capture physical objects, document-images contain a significant amount of text with critical semantics and complicated layouts. In this work, we devise a generic unsupervised technique to learn multimodal affinities between textual entities in a document-image, considering their visual style, the content of their underlying text and their geometric context within the image. We then use these learned affinities to automatically cluster the textual entities in the image into different semantic groups. The core of our approach is a deep optimization scheme dedicated for an image provided by the user that detects and leverages reliable pairwise connections in the multimodal representation of the textual elements in order to properly learn the affinities. We show that our technique can operate on highly varying images spanning a wide range of documents and demonstrate its applicability for various editing operations manipulating the content, appearance and geometry of the image.},
  img={multimodal.jpg},
  code={https://github.com/amzn/multimodal-affinities},
  selected={true}
}


@InProceedings{10.1007/978-3-030-58610-2_13,
author="Markovitz, Amir
and Lavi, Inbal
and Perel, Or
and Mazor, Shai
and Litman, Roee",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Can You Read Me Now? Content Aware Rectification Using Angle Supervision",
booktitle="ECCV",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="208--223",
abstract="The ubiquity of smartphone cameras has led to more and more documents being captured by cameras rather than scanned. Unlike flatbed scanners, photographed documents are often folded and crumpled, resulting in large local variance in text structure. The problem of document rectification is fundamental to the Optical Character Recognition (OCR) process on documents, and its ability to overcome geometric distortions significantly affects recognition accuracy. Despite the great progress in recent OCR systems, most still rely on a pre-process that ensures the text lines are straight and axis aligned. Recent works have tackled the problem of rectifying document images taken in-the-wild using various supervision signals and alignment means. However, they focused on global features that can be extracted from the document's boundaries, ignoring various signals that could be obtained from the document's content.",
isbn="978-3-030-58610-2",
url={https://link.springer.com/chapter/10.1007/978-3-030-58610-2_13},
pdf={https://arxiv.org/pdf/2008.02231.pdf},
img={crease.jpg},
selected={true}
}


@inproceedings{DBLP:conf/cvpr/PatilBPA20,
  author    = {Akshay Gadi Patil and
               Omri Ben{-}Eliezer and
               Or Perel and
               Hadar Averbuch{-}Elor},
  title     = {{READ:} Recursive Autoencoders for Document Layout Generation},
  booktitle = {CVPR Text and Documents in the Deep Learning Era Workshop},
  pages     = {2316--2325},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/CVPRW50498.2020.00280},
  doi       = {10.1109/CVPRW50498.2020.00280},
  timestamp = {Thu, 06 Aug 2020 14:02:38 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/PatilBPA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abbr={Best Paper},
  pdf={https://openaccess.thecvf.com/content_CVPRW_2020/papers/w34/Patil_READ_Recursive_Autoencoders_for_Document_Layout_Generation_CVPRW_2020_paper.pdf},
  abstract="Layout is a fundamental component of any graphic design. Creating large varieties of plausible document layouts can be a tedious task, requiring numerous constraints to be satisfied, including local ones relating different semantic elements and global constraints on the general appearance and spacing. In this paper, we present a novel framework, coined READ, for REcursive Autoencoders for Document layout generation, to generate plausible 2D layouts of documents in large quantities and varieties. First, we devise an exploratory recursive method to extract a structural decomposition of a single document. Leveraging a dataset of documents annotated with labeled bounding boxes, our recursive neural network learns to map the structural representation, given in the form of a simple hierarchy, to a compact code, the space of which is approximated by a Gaussian distribution. Novel hierarchies can be sampled from this space, obtaining new document layouts. Moreover, we introduce a combinatorial metric to measure structural similarity among document layouts. We deploy it to show that our method is able to generate highly variable and realistic layouts. We further demonstrate the utility of our generated layouts in the context of standard detection tasks on documents, showing that detection performance improves when the training data is augmented with generated documents whose layouts are produced by READ.",
  img={read.jpg},
  selected={true}
}
