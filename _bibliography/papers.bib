---
---

@string{aps = {American Physical Society,}}

@article{hertz2022spaghetti, 
         title={SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation}, 
         author={Hertz, Amir and Perel, Or and Giryes, Raja and Sorkine-Hornung, Olga and Cohen-Or, Daniel}, 
         journal={SIGGRAPH}, 
         year={2022},
         archivePrefix={arXiv},
         url={https://arxiv.org/abs/2201.13168},
         pdf={https://arxiv.org/abs/2201.13168.pdf},
         abstract={Neural implicit fields are quickly emerging as an attractive representation for learning based techniques. However, adopting them for 3D shape modeling and editing is challenging. We introduce a method for Editing Implicit Shapes Through Part Aware GeneraTion, permuted in short as SPAGHETTI. Our architecture allows for manipulation of implicit shapes by means of transforming, interpolating and combining shape segments together, without requiring explicit part supervision. SPAGHETTI disentangles shape part representation into extrinsic and intrinsic geometric information. This characteristic enables a generative framework with part-level control. The modeling capabilities of SPAGHETTI are demonstrated using an interactive graphical interface, where users can directly edit neural implicit shapes.},
         img={spaghetti.png},
         code={https://github.com/amirhertz/spaghetti},
         website={https://amirhertz.github.io/spaghetti/},
         selected={true}
}

@article{hertz2021meshdrape,
      title={Mesh Draping: Parametrization-Free Neural Mesh Transfer}, 
      author={Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
      journal={arxiv},
      year={2021},
      eprint={2110.05433},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2110.05433},
      pdf={https://arxiv.org/abs/2110.05433.pdf},
      abstract={Despite recent advances in geometric modeling, 3D mesh modeling still involves a considerable amount of manual labor by experts. In this paper, we introduce Mesh Draping: a neural method for transferring existing mesh structure from one shape to another. The method drapes the source mesh over the target geometry and at the same time seeks to preserve the carefully designed characteristics of the source mesh. At its core, our method deforms the source mesh using progressive positional encoding. We show that by leveraging gradually increasing frequencies to guide the neural optimization, we are able to achieve stable and high quality mesh transfer. Our approach is simple and requires little user guidance, compared to contemporary surface mapping techniques which rely on parametrization or careful manual tuning. Most importantly, Mesh Draping is a parameterization-free method, and thus applicable to a variety of target shape representations, including point clouds, polygon soups, and non-manifold meshes. We demonstrate that the transferred meshing remains faithful to the source mesh design characteristics, and at the same time fits the target geometry well.},
      img={meshdrape.jpg},
      selected={true}
}

@article{hertz2021sape,
      title={SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization}, 
      author={Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
      journal={NeurIPS},
      year={2021},
      eprint={2104.09125},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2104.09125},
      pdf={https://arxiv.org/pdf/2104.09125.pdf},
      abstract={Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.},
      img={sape.jpg},
      code={https://github.com/amirhertz/SAPE},
      website={https://amirhertz.github.io/sape/},
      selected={true}
}

@article{DBLP:journals/corr/abs-2103-10139,
  author    = {Or Perel and
               Oron Anschel and
               Omri Ben{-}Eliezer and
               Shai Mazor and
               Hadar Averbuch{-}Elor},
  title     = {Learning Multimodal Affinities for Textual Editing in Images},
  journal   = {Transaction on Graphics, Presented at SIGGRAPH},
  volume    = {abs/2103.10139},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.10139},
  pdf       = {https://arxiv.org/pdf/2103.10139.pdf},
  archivePrefix = {arXiv},
  eprint    = {2103.10139},
  timestamp = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-10139.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract  = {Nowadays, as cameras are rapidly adopted in our daily routine, images of documents are becoming both abundant and prevalent. Unlike natural images that capture physical objects, document-images contain a significant amount of text with critical semantics and complicated layouts. In this work, we devise a generic unsupervised technique to learn multimodal affinities between textual entities in a document-image, considering their visual style, the content of their underlying text and their geometric context within the image. We then use these learned affinities to automatically cluster the textual entities in the image into different semantic groups. The core of our approach is a deep optimization scheme dedicated for an image provided by the user that detects and leverages reliable pairwise connections in the multimodal representation of the textual elements in order to properly learn the affinities. We show that our technique can operate on highly varying images spanning a wide range of documents and demonstrate its applicability for various editing operations manipulating the content, appearance and geometry of the image.},
  img={multimodal.jpg},
  code={https://github.com/amzn/multimodal-affinities},
  selected={true}
}


@InProceedings{10.1007/978-3-030-58610-2_13,
author="Markovitz, Amir
and Lavi, Inbal
and Perel, Or
and Mazor, Shai
and Litman, Roee",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Can You Read Me Now? Content Aware Rectification Using Angle Supervision",
booktitle="ECCV",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="208--223",
abstract="The ubiquity of smartphone cameras has led to more and more documents being captured by cameras rather than scanned. Unlike flatbed scanners, photographed documents are often folded and crumpled, resulting in large local variance in text structure. The problem of document rectification is fundamental to the Optical Character Recognition (OCR) process on documents, and its ability to overcome geometric distortions significantly affects recognition accuracy. Despite the great progress in recent OCR systems, most still rely on a pre-process that ensures the text lines are straight and axis aligned. Recent works have tackled the problem of rectifying document images taken in-the-wild using various supervision signals and alignment means. However, they focused on global features that can be extracted from the document's boundaries, ignoring various signals that could be obtained from the document's content.",
isbn="978-3-030-58610-2",
url={https://link.springer.com/chapter/10.1007/978-3-030-58610-2_13},
pdf={https://arxiv.org/pdf/2008.02231.pdf},
img={crease.jpg},
selected={true}
}


@inproceedings{DBLP:conf/cvpr/PatilBPA20,
  author    = {Akshay Gadi Patil and
               Omri Ben{-}Eliezer and
               Or Perel and
               Hadar Averbuch{-}Elor},
  title     = {{READ:} Recursive Autoencoders for Document Layout Generation},
  booktitle = {CVPR Text and Documents in the Deep Learning Era Workshop},
  pages     = {2316--2325},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/CVPRW50498.2020.00280},
  doi       = {10.1109/CVPRW50498.2020.00280},
  timestamp = {Thu, 06 Aug 2020 14:02:38 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/PatilBPA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abbr={Best Paper},
  pdf={https://openaccess.thecvf.com/content_CVPRW_2020/papers/w34/Patil_READ_Recursive_Autoencoders_for_Document_Layout_Generation_CVPRW_2020_paper.pdf},
  abstract="Layout is a fundamental component of any graphic design. Creating large varieties of plausible document layouts can be a tedious task, requiring numerous constraints to be satisfied, including local ones relating different semantic elements and global constraints on the general appearance and spacing. In this paper, we present a novel framework, coined READ, for REcursive Autoencoders for Document layout generation, to generate plausible 2D layouts of documents in large quantities and varieties. First, we devise an exploratory recursive method to extract a structural decomposition of a single document. Leveraging a dataset of documents annotated with labeled bounding boxes, our recursive neural network learns to map the structural representation, given in the form of a simple hierarchy, to a compact code, the space of which is approximated by a Gaussian distribution. Novel hierarchies can be sampled from this space, obtaining new document layouts. Moreover, we introduce a combinatorial metric to measure structural similarity among document layouts. We deploy it to show that our method is able to generate highly variable and realistic layouts. We further demonstrate the utility of our generated layouts in the context of standard detection tasks on documents, showing that detection performance improves when the training data is augmented with generated documents whose layouts are produced by READ.",
  img={read.jpg},
  selected={true}
}
