<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Or  Perel</title>
<meta name="description" content="Or Perel's personal page.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Or  Perel
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/profile2.jpg">
      
      
        <div class="address">
          <p></p> <p></p> <p></p>

        </div>
      
    </div>
    

    <div class="clearfix">
      <p>I am a Research Engineer at <a href="https://nv-tlabs.github.io/" target="\_blank">NVIDIA, Toronto AI Lab</a> and a Ph.D. student at the <a href="https://web.cs.toronto.edu/">University of Toronto</a>, advised by <a href="https://www.cs.utoronto.ca/~fidler/">Prof. Sanja Fidler</a> and <a href="https://shumash.com/research">Dr. Masha Shugrina</a>. I am also affiliated with <a href="https://vectorinstitute.ai/">Vector Institute</a>.</p>

<p>Previously in my career I worked as an Applied Scientist at <a href="https://aws.amazon.com/rekognition/" target="\_blank">Amazon Rekognition</a>, as a Research Engineer at <a href="https://www.autodesk.com/" target="\_blank">Autodesk</a> and as a System Architect and Software Engineer at <a href="https://en.wikipedia.org/wiki/Ofek_unit" target="\_blank">Ofek, IAF</a>. I’ve obtained my M.Sc. in Computer Sciences from Tel Aviv University, under the supervision of <a href="https://www.cs.tau.ac.il/~dcor/index.html" target="\_blank">Prof. Daniel Cohen-Or</a>. I am also a <a href="https://www.mamram.tech/" target="\_blank">Mamram</a> Alumni.</p>

<p>My research lies at the convergence of computer vision, graphics, and machine learning. I am particularly interested in AI-driven 3D simulations, including realistic reconstructions and interactive workflows for manipulating them.</p>

    </div>

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%6F%72%72.%70%65%72%65%6C@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=E2y2UqYAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/orperel" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/or-perel-50a27694" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/OrPerel" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>










      </div>
      <div class="contact-note"></div>
    </div>
    

    

    
      <div class="publications">
  <h2>Publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    <!-- 
    <abbr class="badge"></abbr>
     -->
  
    
    <!-- <div class="hidden-xs"> -->
    <div class="d-none d-lg-block">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/3dgrt.png">  
    </div>
  
  </div>

  <div id="moenneloccoz20243dgaussianraytracing" class="col-sm-8">
    

      

      <div class="title">3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Nicolas Moenne-Loccoz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ashkan Mirzaei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Or Perel</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Riccardo Lutio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Janick Martinez Esturo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gavriel State,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sanja Fidler,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nicholas Sharp,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zan Gojcic
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>SIGGRAPH Asia</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/abs/2407.07090.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://gaussiantracer.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Particle-based representations of radiance fields such as 3D Gaussian Splatting have found great success for reconstructing and re-rendering of complex scenes. Most existing methods render particles via rasterization, projecting them to screen space tiles for processing in a sorted order. This work instead considers ray tracing the particles, building a bounding volume hierarchy and casting a ray for each pixel using high-performance GPU ray tracing hardware. To efficiently handle large numbers of semi-transparent particles, we describe a specialized rendering algorithm which encapsulates particles with bounding meshes to leverage fast ray-triangle intersections, and shades batches of intersections in depth-order. The benefits of ray tracing are well-known in computer graphics: processing incoherent rays for secondary lighting effects such as shadows and reflections, rendering from highly-distorted cameras common in robotics, stochastically sampling rays, and more. With our renderer, this flexibility comes at little cost compared to rasterization. Experiments demonstrate the speed and accuracy of our approach, as well as several applications in computer graphics and vision. We further propose related improvements to the basic Gaussian representation, including a simple use of generalized kernel functions which significantly reduces particle hit counts.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <!-- 
    <abbr class="badge"></abbr>
     -->
  
    
    <!-- <div class="hidden-xs"> -->
    <div class="d-none d-lg-block">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/simplicits.gif">  
    </div>
  
  </div>

  <div id="modi2024Simplicits" class="col-sm-8">
    

      

      <div class="title">Simplicits: Mesh-Free, Geometry-Agnostic, Elastic Simulation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Vismay Modi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nicholas Sharp,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Or Perel</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shinjiro Sueda,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and David I. W. Levin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>SIGGRAPH</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/abs/2407.09497.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://research.nvidia.com/labs/toronto-ai/simplicits/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The proliferation of 3D representations, from explicit meshes to implicit neural fields and more, motivates the need for simulators agnostic to representation. We present a data-, mesh-, and grid-free solution for elastic simulation for any object in any geometric representation undergoing large, nonlinear deformations. We note that every standard geometric representation can be reduced to an occupancy function queried at any point in space, and we define a simulator atop this common interface. For each object, we fit a small implicit neural network encoding spatially varying weights that act as a reduced deformation basis. These weights are trained to learn physically significant motions in the object via random perturbations. Our loss ensures we find a weight-space basis that best minimizes deformation energy by stochastically evaluating elastic energies through Monte Carlo sampling of the deformation volume. At runtime, we simulate in the reduced basis and sample the deformations back to the original domain. Our experiments demonstrate the versatility, accuracy, and speed of this approach on data including signed distance functions, point clouds, neural primitives, tomography scans, radiance fields, Gaussian splats, surface meshes, and volume meshes, as well as showing a variety of material energies, contact models, and time integration schemes.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <!-- 
    <abbr class="badge"></abbr>
     -->
  
    
    <!-- <div class="hidden-xs"> -->
    <div class="d-none d-lg-block">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/sked.png">  
    </div>
  
  </div>

  <div id="mikaeili2023sked" class="col-sm-8">
    

      

      <div class="title">SKED: Sketch-guided Text-based 3D Editing</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Aryan Mikaeili,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Or Perel</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mehdi Safaee,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Daniel Cohen-Or,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Ali Mahdavi-Amiri
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ICCV</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/abs/2303.10735.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://sked-paper.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Text-to-image diffusion models are gradually introduced into computer graphics, recently enabling the development of Text-to-3D pipelines in an open domain. However, for interactive editing purposes, local manipulations of content through a simplistic textual interface can be arduous. Incorporating user guided sketches with Text-to-image pipelines offers users more intuitive control. Still, as state-of-the-art Text-to-3D pipelines rely on optimizing Neural Radiance Fields (NeRF) through gradients from arbitrary rendering views, conditioning on sketches is not straightforward. In this paper, we present SKED, a technique for editing 3D shapes represented by NeRFs. Our technique utilizes as few as two guiding sketches from different views to alter an existing neural field. The edited region respects the prompt semantics through a pre-trained diffusion model. To ensure the generated output adheres to the provided sketches, we propose novel loss functions to generate the desired edits while preserving the density and radiance of the base instance. We demonstrate the effectiveness of our proposed method through several qualitative and quantitative experiments.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <!-- 
    <abbr class="badge"></abbr>
     -->
  
    
    <!-- <div class="hidden-xs"> -->
    <div class="d-none d-lg-block">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/spaghetti.png">  
    </div>
  
  </div>

  <div id="hertz2022spaghetti" class="col-sm-8">
    

      

      <div class="title">SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Amir Hertz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Or Perel</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Raja Giryes,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Olga Sorkine-Hornung,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Daniel Cohen-Or
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>SIGGRAPH</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/abs/2201.13168.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/amirhertz/spaghetti" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://amirhertz.github.io/spaghetti/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Neural implicit fields are quickly emerging as an attractive representation for learning based techniques. However, adopting them for 3D shape modeling and editing is challenging. We introduce a method for Editing Implicit Shapes Through Part Aware GeneraTion, permuted in short as SPAGHETTI. Our architecture allows for manipulation of implicit shapes by means of transforming, interpolating and combining shape segments together, without requiring explicit part supervision. SPAGHETTI disentangles shape part representation into extrinsic and intrinsic geometric information. This characteristic enables a generative framework with part-level control. The modeling capabilities of SPAGHETTI are demonstrated using an interactive graphical interface, where users can directly edit neural implicit shapes.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <!-- 
    <abbr class="badge"></abbr>
     -->
  
    
    <!-- <div class="hidden-xs"> -->
    <div class="d-none d-lg-block">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/meshdrape.jpg">  
    </div>
  
  </div>

  <div id="hertz2021meshdrape" class="col-sm-8">
    

      

      <div class="title">Mesh Draping: Parametrization-Free Neural Mesh Transfer</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Amir Hertz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Or Perel</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Raja Giryes,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Olga Sorkine-Hornung,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Daniel Cohen-Or
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Computer Graphics Forum</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/abs/2110.05433.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite recent advances in geometric modelling, 3D mesh modelling still involves a considerable amount of manual labour by experts. In this paper, we introduce Mesh Draping: a neural method for transferring existing mesh structure from one shape to another. The method drapes the source mesh over the target geometry and at the same time seeks to preserve the carefully designed characteristics of the source mesh. At its core, our method deforms the source mesh using progressive positional encoding (PE). We show that by leveraging gradually increasing frequencies to guide the neural optimization, we are able to achieve stable and high-quality mesh transfer. Our approach is simple and requires little user guidance, compared to contemporary surface mapping techniques which rely on parametrization or careful manual tuning. Most importantly, Mesh Draping is a parameterization-free method, and thus applicable to a variety of target shape representations, including point clouds, polygon soups and non-manifold meshes. We demonstrate that the transferred meshing remains faithful to the source mesh design characteristics, and at the same time fits the target geometry well.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <!-- 
    <abbr class="badge"></abbr>
     -->
  
    
    <!-- <div class="hidden-xs"> -->
    <div class="d-none d-lg-block">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/sape.jpg">  
    </div>
  
  </div>

  <div id="hertz2021sape" class="col-sm-8">
    

      

      <div class="title">SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Amir Hertz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Or Perel</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Raja Giryes,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Olga Sorkine-Hornung,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Daniel Cohen-Or
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>NeurIPS</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2104.09125.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/amirhertz/SAPE" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://amirhertz.github.io/sape/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <!-- 
    <abbr class="badge"></abbr>
     -->
  
    
    <!-- <div class="hidden-xs"> -->
    <div class="d-none d-lg-block">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/multimodal.jpg">  
    </div>
  
  </div>

  <div id="DBLP:journals/corr/abs-2103-10139" class="col-sm-8">
    

      

      <div class="title">Learning Multimodal Affinities for Textual Editing in Images</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Or Perel</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Oron Anschel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Omri Ben-Eliezer,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shai Mazor,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Hadar Averbuch-Elor
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Transaction on Graphics, Presented at SIGGRAPH</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2103.10139.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/amzn/multimodal-affinities" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Nowadays, as cameras are rapidly adopted in our daily routine, images of documents are becoming both abundant and prevalent. Unlike natural images that capture physical objects, document-images contain a significant amount of text with critical semantics and complicated layouts. In this work, we devise a generic unsupervised technique to learn multimodal affinities between textual entities in a document-image, considering their visual style, the content of their underlying text and their geometric context within the image. We then use these learned affinities to automatically cluster the textual entities in the image into different semantic groups. The core of our approach is a deep optimization scheme dedicated for an image provided by the user that detects and leverages reliable pairwise connections in the multimodal representation of the textual elements in order to properly learn the affinities. We show that our technique can operate on highly varying images spanning a wide range of documents and demonstrate its applicability for various editing operations manipulating the content, appearance and geometry of the image.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <!-- 
    <abbr class="badge"></abbr>
     -->
  
    
    <!-- <div class="hidden-xs"> -->
    <div class="d-none d-lg-block">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/crease.jpg">  
    </div>
  
  </div>

  <div id="10.1007/978-3-030-58610-2_13" class="col-sm-8">
    

      

      <div class="title">Can You Read Me Now? Content Aware Rectification Using Angle Supervision</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Amir Markovitz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Inbal Lavi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Or Perel</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shai Mazor,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Roee Litman
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ECCV</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2008.02231.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The ubiquity of smartphone cameras has led to more and more documents being captured by cameras rather than scanned. Unlike flatbed scanners, photographed documents are often folded and crumpled, resulting in large local variance in text structure. The problem of document rectification is fundamental to the Optical Character Recognition (OCR) process on documents, and its ability to overcome geometric distortions significantly affects recognition accuracy. Despite the great progress in recent OCR systems, most still rely on a pre-process that ensures the text lines are straight and axis aligned. Recent works have tackled the problem of rectifying document images taken in-the-wild using various supervision signals and alignment means. However, they focused on global features that can be extracted from the document’s boundaries, ignoring various signals that could be obtained from the document’s content.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    <!-- 
    <abbr class="badge">Best Paper</abbr>
     -->
  
    
    <!-- <div class="hidden-xs"> -->
    <div class="d-none d-lg-block">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/read.jpg">  
    </div>
  
  </div>

  <div id="DBLP:conf/cvpr/PatilBPA20" class="col-sm-8">
    

      
      <div class="abbr">
      
        <abbr class="badge">Best Paper</abbr>
      
      </div>
      

      <div class="title">READ: Recursive Autoencoders for Document Layout Generation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Akshay Gadi Patil,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Omri Ben-Eliezer,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Or Perel</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Hadar Averbuch-Elor
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR Text and Documents in the Deep Learning Era Workshop</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w34/Patil_READ_Recursive_Autoencoders_for_Document_Layout_Generation_CVPRW_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Layout is a fundamental component of any graphic design. Creating large varieties of plausible document layouts can be a tedious task, requiring numerous constraints to be satisfied, including local ones relating different semantic elements and global constraints on the general appearance and spacing. In this paper, we present a novel framework, coined READ, for REcursive Autoencoders for Document layout generation, to generate plausible 2D layouts of documents in large quantities and varieties. First, we devise an exploratory recursive method to extract a structural decomposition of a single document. Leveraging a dataset of documents annotated with labeled bounding boxes, our recursive neural network learns to map the structural representation, given in the form of a simple hierarchy, to a compact code, the space of which is approximated by a Gaussian distribution. Novel hierarchies can be sampled from this space, obtaining new document layouts. Moreover, we introduce a combinatorial metric to measure structural similarity among document layouts. We deploy it to show that our method is able to generate highly variable and realistic layouts. We further demonstrate the utility of our generated layouts in the context of standard detection tasks on documents, showing that detection performance improves when the training data is augmented with generated documents whose layouts are produced by READ.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
</div>

    

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Or  Perel.
    
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
